{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmark: LandmarkTriangulation vs t-SNE\n",
    "\n",
    "This notebook compares **LandmarkTriangulation** against scikit-learn's **t-SNE** on synthetic clustered data.\n",
    "\n",
    "**Objectives:**\n",
    "- Compare execution time across different landmark selection strategies\n",
    "- Evaluate clustering quality using silhouette scores\n",
    "- Visualize embeddings for qualitative assessment\n",
    "\n",
    "**Dataset:** 2,000 samples, 50 features, 5 clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports\n",
    "\n",
    "Install the package first if needed:\n",
    "```bash\n",
    "uv sync --extra examples\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "from landmark_triangulation import LandmarkTriangulation\n",
    "\n",
    "print(\"âœ“ All imports successful\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Helper Functions\n",
    "\n",
    "Define utilities for data generation and benchmarking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_synthetic_data(n_samples=2000, n_features=50, n_clusters=5):\n",
    "    \"\"\"\n",
    "    Generate high-dimensional synthetic data with distinct clusters.\n",
    "    \n",
    "    Returns:\n",
    "        X: Feature matrix of shape (n_samples, n_features)\n",
    "        y: Cluster labels of shape (n_samples,)\n",
    "    \"\"\"\n",
    "    print(f\"Generating {n_samples} samples with {n_features} features and {n_clusters} clusters...\")\n",
    "    \n",
    "    X, y = make_blobs(\n",
    "        n_samples=n_samples,\n",
    "        n_features=n_features,\n",
    "        centers=n_clusters,\n",
    "        cluster_std=2.0,\n",
    "        random_state=42,\n",
    "    )\n",
    "    return X, y\n",
    "\n",
    "\n",
    "def run_method(name, model, X, y):\n",
    "    \"\"\"\n",
    "    Run a dimensionality reduction method and compute metrics.\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with method name, runtime, silhouette score, and embedding.\n",
    "    \"\"\"\n",
    "    print(f\"Running {name}...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        X_embedded = model.fit_transform(X)\n",
    "        duration = time.time() - start_time\n",
    "        \n",
    "        # Silhouette score: measures cluster separation (-1 to 1, higher is better)\n",
    "        score = silhouette_score(X_embedded, y)\n",
    "        \n",
    "        print(f\"  âœ“ Finished in {duration:.2f}s | Silhouette: {score:.3f}\")\n",
    "        \n",
    "        return {\n",
    "            \"Method\": name,\n",
    "            \"Time (s)\": duration,\n",
    "            \"Silhouette\": score,\n",
    "            \"Embedding\": X_embedded,\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"  âœ— Failed: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Generate Dataset\n",
    "\n",
    "Create synthetic data matching the README benchmark specifications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic dataset\n",
    "X, y = generate_synthetic_data(\n",
    "    n_samples=2000, \n",
    "    n_features=50, \n",
    "    n_clusters=5\n",
    ")\n",
    "\n",
    "print(f\"\\nData shape: {X.shape}\")\n",
    "print(f\"Number of clusters: {len(set(y))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Configure Methods\n",
    "\n",
    "Set up all dimensionality reduction methods to benchmark:\n",
    "- **Random mode**: Randomly samples landmarks from data\n",
    "- **Synthetic mode**: Generates sine-wave landmarks\n",
    "- **Hybrid mode**: Generates synthetic landmarks and snaps to nearest real points\n",
    "- **t-SNE**: Baseline comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define methods to benchmark\n",
    "methods = [\n",
    "    (\n",
    "        \"Random Mode\",\n",
    "        LandmarkTriangulation(\n",
    "            n_landmarks=150, \n",
    "            landmark_mode=\"random\", \n",
    "            random_state=42\n",
    "        ),\n",
    "    ),\n",
    "    (\n",
    "        \"Synthetic Mode\",\n",
    "        LandmarkTriangulation(\n",
    "            n_landmarks=150, \n",
    "            landmark_mode=\"synthetic\", \n",
    "            random_state=42\n",
    "        ),\n",
    "    ),\n",
    "    (\n",
    "        \"Hybrid Mode\",\n",
    "        LandmarkTriangulation(\n",
    "            n_landmarks=150, \n",
    "            landmark_mode=\"hybrid\", \n",
    "            random_state=42\n",
    "        ),\n",
    "    ),\n",
    "    (\n",
    "        \"t-SNE\",\n",
    "        TSNE(\n",
    "            n_components=2, \n",
    "            init=\"pca\", \n",
    "            learning_rate=\"auto\", \n",
    "            random_state=42\n",
    "        ),\n",
    "    ),\n",
    "]\n",
    "\n",
    "print(f\"Prepared {len(methods)} methods for benchmarking\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Run Benchmark\n",
    "\n",
    "Execute all methods and collect timing and quality metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run all methods and collect results\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"BENCHMARK START\")\n",
    "print(\"=\" * 60 + \"\\n\")\n",
    "\n",
    "results = []\n",
    "for name, model in methods:\n",
    "    res = run_method(name, model, X, y)\n",
    "    if res is not None:\n",
    "        results.append(res)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"BENCHMARK COMPLETE\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Results Summary\n",
    "\n",
    "Display performance metrics and identify the fastest method and best clustering quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create summary table\n",
    "df_results = pd.DataFrame(results)[[\"Method\", \"Time (s)\", \"Silhouette\"]]\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"RESULTS SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "print(df_results.to_string(index=False))\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Highlight fastest and best quality\n",
    "fastest = df_results.loc[df_results[\"Time (s)\"].idxmin()]\n",
    "best_quality = df_results.loc[df_results[\"Silhouette\"].idxmax()]\n",
    "\n",
    "print(f\"\\nâš¡ Fastest: {fastest['Method']} ({fastest['Time (s)']:.2f}s)\")\n",
    "print(f\"ðŸŽ¯ Best Quality: {best_quality['Method']} (Silhouette: {best_quality['Silhouette']:.3f})\")\n",
    "\n",
    "# Calculate speedup\n",
    "tsne_time = df_results[df_results[\"Method\"] == \"t-SNE\"][\"Time (s)\"].values[0]\n",
    "for _, row in df_results.iterrows():\n",
    "    if row[\"Method\"] != \"t-SNE\":\n",
    "        speedup = tsne_time / row[\"Time (s)\"]\n",
    "        print(f\"ðŸ“Š {row['Method']}: {speedup:.1f}x faster than t-SNE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Visualization\n",
    "\n",
    "Compare embeddings visually across all methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison plot\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10), constrained_layout=True)\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, res in enumerate(results):\n",
    "    ax = axes[i]\n",
    "    emb = res[\"Embedding\"]\n",
    "\n",
    "    scatter = ax.scatter(\n",
    "        emb[:, 0],\n",
    "        emb[:, 1],\n",
    "        c=y,\n",
    "        cmap=\"viridis\",\n",
    "        alpha=0.6,\n",
    "        s=8,\n",
    "        edgecolors=\"none\",\n",
    "    )\n",
    "\n",
    "    ax.set_title(\n",
    "        f\"{res['Method']}\\n\"\n",
    "        f\"Time: {res['Time (s)']:.2f}s | Silhouette: {res['Silhouette']:.3f}\",\n",
    "        fontsize=11,\n",
    "    )\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    ax.grid(True, alpha=0.2, linestyle=\"--\")\n",
    "\n",
    "# Add a single colorbar for the whole figure\n",
    "fig.colorbar(scatter, ax=axes.ravel().tolist(), label=\"Cluster\")\n",
    "\n",
    "fig.suptitle(\n",
    "    \"Dimensionality Reduction Benchmark: LandmarkTriangulation vs t-SNE\",\n",
    "    fontsize=14,\n",
    "    fontweight=\"bold\",\n",
    ")\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
